{"cells":[{"cell_type":"markdown","metadata":{"id":"8dqPSoz0n6Fk"},"source":["Some code and ideas from\n","https://github.com/lukysummer/SkipGram_with_NegativeSampling_Pytorch/tree/master\n","\n","and https://github.com/reynoldsnlp/F23_LING581/blob/main/code/ch6_numpy.py"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1702761832453,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"},"user_tz":420},"id":"zdi2XZPqpBaN","outputId":"d2a80a60-8c4b-4f99-f04f-e334eea77863"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"]}],"source":["import os, pickle, json\n","from collections import Counter\n","from itertools import islice\n","from pprint import pprint as pp\n","\n","import numpy as np\n","\n","import torch\n","from torch import tensor, nn, optim\n","from torch.nn import functional as F\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n","\n","import nltk\n","from nltk.corpus import brown\n","\n","nltk.download('brown')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17201,"status":"ok","timestamp":1702761513060,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"},"user_tz":420},"id":"2gRIj-eVzMSE","outputId":"84f555dd-3672-482f-9e8d-4dd0bd2739b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","DRIVE_PATH = '/content/gdrive/MyDrive/School/LING581/final'"]},{"cell_type":"markdown","metadata":{"id":"bhgAgV0oovRC"},"source":["# Parameters"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Tlzv34AlyL1z","executionInfo":{"status":"ok","timestamp":1702761941722,"user_tz":420,"elapsed":2,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["# Vocab parameters\n","# If any of this changes, will need to rebuild context and\n","# negative sample lists.\n","min_freq = 5\n","UNK = '<UNK>'\n","PAD = '<PAD>'\n","specials = (UNK, PAD)\n","\n","# Window parameters\n","window_size = 5\n","edge = (window_size - 1) // 2\n","\n","# Negative sample parameters\n","num_negs_per_pos = 2\n","alpha = 0.75\n","\n","# Set to false to re-compute and save.\n","load_vocab = False\n","load_contexts = True\n","load_negs = True\n","\n","# Paths\n","vocab_file_path = 'vocab'\n","contexts_file_path = 'contexts'\n","negs_file_path = 'negative-samples'\n","embedding_folder_path = 'embedding-models'\n","epoch_losses_path = 'train-embeddings_epoch-losses'"]},{"cell_type":"markdown","metadata":{"id":"UfD7Dd0HoxjU"},"source":["# Build vocab, contexts, and negative samples"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4787,"status":"ok","timestamp":1702761517845,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"},"user_tz":420},"id":"zgHBp_7ul3qx","outputId":"c84ebdb5-7eab-47f9-c74d-7cbfacbc564d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0, 1, 16], ['<UNK>', '<PAD>', 'The'])"]},"metadata":{},"execution_count":4}],"source":["### Build vocab\n","abs_vocab_file_path = os.path.join(DRIVE_PATH, vocab_file_path + '.pt')\n","\n","if load_vocab:\n","  vocab = torch.load(abs_vocab_file_path)\n","else:\n","  vocab = build_vocab_from_iterator(brown.sents(), min_freq=min_freq,\n","                                    specials=specials)\n","  vocab.set_default_index(vocab[UNK])\n","\n","  torch.save(vocab, abs_vocab_file_path)\n","\n","# Demonstrate vocab token2id and id2token maps\n","_ = vocab.lookup_indices([UNK, PAD, 'The'])\n","_, vocab.lookup_tokens(_)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dM1hCZ5W1ZT3","executionInfo":{"status":"ok","timestamp":1702761521507,"user_tz":420,"elapsed":3665,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["### Convert the list of sentences from strings to ids.\n","id_sents = [None] * len(brown.sents())\n","for i, sent in enumerate(brown.sents()):\n","  id_sents[i] = vocab.lookup_indices(sent)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"W_0mGZgQrqrp","executionInfo":{"status":"ok","timestamp":1702761524079,"user_tz":420,"elapsed":2581,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["### Prepare contexts.\n","# In each row (sublist), the first word is the target,\n","# and the remaining words are the contexts.\n","abs_contexts_file_path = os.path.join(DRIVE_PATH, contexts_file_path)\n","\n","if load_contexts:\n","  with open (abs_contexts_file_path, 'rb') as file:\n","    contexts = pickle.load(file)\n","else:\n","  contexts = list()\n","  num_contexts = list()\n","\n","  for sent in id_sents:\n","    length = len(sent)\n","    for i, token in enumerate(sent):\n","      contexts.append([token] + sent[max(0, i-edge) : i] +\n","                      sent[i+1 : min(i+1 + edge, length)])\n","      num_contexts.append(len(contexts[-1]) - 1)\n","\n","  with open(abs_contexts_file_path, 'wb') as file:\n","    pickle.dump(contexts, file)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zbMoh9WzeKv9","executionInfo":{"status":"ok","timestamp":1702761531771,"user_tz":420,"elapsed":7694,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["### Prepare negative samples.\n","abs_negs_file_path = os.path.join(DRIVE_PATH, negs_file_path)\n","\n","if load_negs:\n","  with open (abs_negs_file_path, 'rb') as file:\n","    negative_samples = pickle.load(file)\n","else:\n","  # Flattened corpus of ids\n","  id_words = [ token for sent in id_sents for token in sent ]\n","\n","  # Get frequencies of token IDs.\n","  freqs = Counter(id_words)\n","\n","  # Get weighted unigram frequencies from which to get negative samples,\n","  # as in section 6.8.2 of the text.\n","  weighted = Counter({ token : freq**alpha for token, freq in freqs.items() })\n","\n","  # Turn into probabilities\n","  total = weighted.total()\n","  weights = { token : freq / total for token, freq in weighted.items() }\n","  ids, probabilities = list(weights.keys()), list(weights.values())\n","\n","  # Calculate the number of negative samples to get\n","  # (using num_negs_per_pos per context).\n","  num_negs = sum(num_contexts) * num_negs_per_pos\n","\n","  # Get flattened list of negative samples.\n","  flat_negative_samples = np.random.choice(ids, p=probabilities, size=num_negs)\n","  iter_negative_samples = iter(flat_negative_samples)\n","\n","  # Structure negative samples like the contexts (each row contains the target\n","  # followed by the negative samples).\n","  negative_samples = [[context[0]] +\n","                      list(islice(iter_negative_samples, num * num_negs_per_pos))\n","                      for context, num in zip(contexts, num_contexts)]\n","\n","  # Make sure no token has itself as a negative sample\n","  # Note: this takes about 2 minutes\n","  print('Total iterations:', len(num_contexts))\n","  for i, num in enumerate(num_contexts):\n","    while negative_samples[i][0] in negative_samples[i][1:]:\n","      negative_samples[i][1:] = np.random.choice(ids, p=probabilities,\n","                                                size=num * num_negs_per_pos)\n","    if i % 100000 == 0:\n","      print(i)\n","\n","  # This takes about 30 seconds.\n","  with open(abs_negs_file_path, 'wb') as file:\n","    pickle.dump(negative_samples, file)"]},{"cell_type":"markdown","metadata":{"id":"bTvn6rVVo1Az"},"source":["# Build dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"IS9-ByHgFZBA","executionInfo":{"status":"ok","timestamp":1702761531771,"user_tz":420,"elapsed":12,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["class Samples(Dataset):\n","  '''Dataset that returns a tuple of a contexts list and an corresponding\n","  negative samples list.\n","  '''\n","\n","  def __init__(self, contexts, negative_samples):\n","    self.contexts = contexts\n","    self.negative_samples = negative_samples\n","\n","  def __len__(self):\n","    return len(self.contexts)\n","\n","  def __getitem__(self, idx):\n","    return self.contexts[idx], self.negative_samples[idx]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"M-5b5opTD4Zf","executionInfo":{"status":"ok","timestamp":1702761531771,"user_tz":420,"elapsed":11,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["def collate_batch(batch):\n","  '''Given a batch of context lists and negative samples lists, create a batch.\n","\n","  Return:\n","    tokens (tensor): the tokens to which contexts and negative samples\n","      correspond\n","    padded_contexts (tensor): padded contexts, each row corresponding to a token\n","      in tokens\n","    padded_negs (tensor): padded negative samples, each row corresponding to a\n","      token in tokens\n","  '''\n","\n","  padding_value = vocab[PAD]\n","\n","  # Get tokens from beginning of each contexts list (could get them from\n","  # negative samples list too)\n","  tokens = tensor([contexts[0] for contexts, _ in batch], dtype=int).to(device)\n","\n","  # Convert lists of context token IDs to tensors\n","  contexts_list = [tensor(contexts[1:], dtype=int) for contexts, _ in batch]\n","  # Pad\n","  padded_contexts = pad_sequence(contexts_list, batch_first=True,\n","                                 padding_value=padding_value).to(device)\n","\n","  # Same for negative samples\n","  negs_list = [tensor(negs[1:], dtype=int) for _, negs in batch]\n","  padded_negs = pad_sequence(negs_list, batch_first=True,\n","                             padding_value=padding_value).to(device)\n","\n","  return tokens, padded_contexts, padded_negs"]},{"cell_type":"markdown","metadata":{"id":"5sZD5fOgR1Z7"},"source":["# Define model"]},{"cell_type":"markdown","metadata":{"id":"5OqjB-cFLef3"},"source":["Discussion about `clip_norm` and clipping to `embedding_dim**(1/2)`:\n","\n","The point is to keep the weights from getting too large.\n","\n","The expected value of the squared 2-norm of a standard multivariate normal random variable is the dimension of the random variable, so we assume the expected value of the 2-norm is approximately the square root of the dimension.\n","\n","For arbitrary $p$-norms, the expected value is inversely related to p and still dependent on the dimension (so larger dimensions and smaller p both contribute to larger expected values). See my \"experiment\" notebook.\n","\n","However, I don't know the exact relation, so this is just to keep the invidual weights from getting too large, yet not clip them to be too small (for example, regardless of $p$, given a vector on a unit ball, the entries will be smaller the larger the dimension).\n","\n","Additional work could normalize vectors to live on a ball of some radius, rather than just clip large vectors."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TSHMKa_ixwPM","executionInfo":{"status":"ok","timestamp":1702761531771,"user_tz":420,"elapsed":11,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["class CustomEmbedding(nn.Module):\n","  def __init__(self, vocab, similarity, embedding_dim, clip_norm):\n","    \"\"\"\n","\n","    Params:\n","      similarity (float or str):\n","        float: the type of p-norm to use when computing similarities\n","        str:\n","          'dot': dot product\n","          'cos': cosine similarity\n","      clip_norm (bool): if True, embeddings with norm larger than\n","        `MAX_NORM_MULTIPLE * max_norm` will be normalized to have this norm\n","    \"\"\"\n","    super().__init__()\n","\n","    MAX_NORM_MULTIPLE = 2\n","\n","    self.vocab = vocab\n","    self.vocab_size = len(vocab)\n","    self.embedding_dim = embedding_dim\n","    self.similarity = similarity\n","\n","    match self.similarity:\n","      case 'dot' | 'cos':\n","        ord = 2\n","      case float() | int():\n","        ord = self.similarity\n","      case _:\n","        raise ValueError('invalid similarity parameter')\n","\n","    if clip_norm:\n","      sample_size = 10000\n","      sample = torch.randn((sample_size, embedding_dim))\n","      mean_norm = sample.norm(dim=1, p=ord).mean()\n","      max_norm = MAX_NORM_MULTIPLE * mean_norm\n","    else:\n","      max_norm = None\n","\n","    self.target_embedding = nn.Embedding(self.vocab_size, self.embedding_dim,\n","                                         max_norm=max_norm, norm_type=ord)\n","    self.context_embedding = nn.Embedding(self.vocab_size, self.embedding_dim,\n","                                          max_norm=max_norm, norm_type=ord)\n","\n","    self.sim = self.get_sim()\n","    self.score = self.get_score()\n","\n","  def get_sim(self):\n","    # Output of sim will be (batch)x(number of contexts/negs)\n","    match self.similarity:\n","      case 'dot':\n","        # Want to maximize dot product for true contexts and\n","        # minimize for negative samples\n","        return lambda target_embs, context_embs: \\\n","          torch.bmm(context_embs, target_embs.unsqueeze(-1)).squeeze(-1)\n","      case 'cos':\n","        # Want to maximize cosine similarity for true contexts and\n","        # minimize for negative samples\n","        return lambda target_embs, context_embs: \\\n","          F.cosine_similarity(target_embs.unsqueeze(1), context_embs, dim=-1)\n","      case float() | int():\n","        # Want to maximize negative distance for true contexts and\n","        # minimize for negative samples\n","        return lambda target_embs, context_embs: \\\n","          torch.linalg.vector_norm(target_embs.unsqueeze(1) - context_embs,\n","                                   dim=-1, ord=self.similarity).neg()\n","      case _:\n","        raise ValueError('invalid similarity parameter')\n","\n","  def get_score(self):\n","    match self.similarity:\n","      case 'dot':\n","        return lambda target_embs, context_embs: \\\n","          torch.mm(target_embs, context_embs.T)\n","      case 'cos':\n","        return self.sim\n","      case float() | int():\n","        return self.sim\n","\n","  def forward(self, batch):\n","    targets, contexts, negs = batch\n","\n","    target_embs = self.target_embedding(targets)\n","    context_embs = self.context_embedding(contexts)\n","    neg_embs = self.context_embedding(negs)\n","\n","    context_scores = self.sim(target_embs, context_embs)\n","    neg_scores = self.sim(target_embs, neg_embs)\n","\n","    return context_scores, neg_scores"]},{"cell_type":"markdown","metadata":{"id":"O4vMLVcNpBWQ"},"source":["# Helper functions for verifying embeddings are training"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"EBxSXmseWeXC","executionInfo":{"status":"ok","timestamp":1702761531771,"user_tz":420,"elapsed":11,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["valid_tokens = ['dog', 'milk', 'run', 'apple', 'hurt']\n","valid_idxs = tensor(vocab.lookup_indices(valid_tokens))\n","\n","def get_topk(k, model, valid_idxs):\n","  with torch.no_grad():\n","    valid_embs = model.target_embedding(valid_idxs.to(device))\n","    target_embs = model.target_embedding.weight\n","\n","    scores = model.score(target_embs, valid_embs)\n","\n","    topk = scores.topk(k, dim=0)[1]\n","\n","    return [vocab.lookup_tokens(topk[:, i].tolist()) for i in range(len(valid_tokens))]\n","\n","def print_topk(k, model, valid_idxs):\n","  '''Get the words with that are closest to valid_tokens according to the model.'''\n","\n","  topk_lists = get_topk(topk, model, valid_idxs)\n","  for token, topk_list in zip(valid_tokens, topk_lists):\n","    print(f'  {token:<7}: ', ' '.join(topk_list))"]},{"cell_type":"markdown","metadata":{"id":"MNKpABljpJHt"},"source":["# Define training procedure"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"8QEkitlGOa6l","executionInfo":{"status":"ok","timestamp":1702762937439,"user_tz":420,"elapsed":163,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["def train_skipgram(model,\n","                   criterion,\n","                   optimizer,\n","                   dataloader,\n","                   num_epochs=5,\n","                   print_every=1500,\n","                   topk=5,):\n","    print_topk(topk, model, valid_idxs)\n","\n","    # List of average batch loss for each epoch\n","    avg_epoch_losses = list()\n","\n","    for epoch in range(num_epochs):\n","      print(f'epoch: {epoch+1}/{num_epochs}')\n","      num_batches = len(dataloader)\n","\n","      model.train()\n","      epoch_loss = 0.\n","      for batch_num, (targets, contexts, negs) in enumerate(dataloader):\n","\n","        context_scores, neg_scores = model((targets, contexts, negs))\n","\n","        scores = torch.cat((context_scores, neg_scores), dim=-1)\n","        labels = torch.cat((torch.ones_like(context_scores),\n","                            torch.zeros_like(neg_scores)), dim=-1)\n","\n","        loss = criterion(scores, labels)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        epoch_loss += loss.item() * (len(targets) / batch_size)\n","\n","        if (batch_num % print_every) == 0:\n","          print(f' batch {batch_num:>6}/{num_batches} | loss {loss.item():<.3f}')\n","\n","      avg_epoch_losses.append(epoch_loss / num_batches)\n","\n","      print_topk(topk, model, valid_idxs)\n","\n","    return avg_epoch_losses"]},{"cell_type":"markdown","metadata":{"id":"dVXjZAgIp0rz"},"source":["# Train"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"F4s1oo8qpyUq","executionInfo":{"status":"ok","timestamp":1702763050315,"user_tz":420,"elapsed":180,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}}},"outputs":[],"source":["# Parameters\n","embedding_dim = 64\n","learning_rate = 0.003\n","batch_size = 256\n","\n","print_every = 1500\n","num_epochs = 5\n","topk = 6\n","\n","criterion = nn.BCEWithLogitsLoss()\n","\n","similarities = ['dot', 'cos', 1, 2, 3]\n","\n","names = [sim if type(sim) is str else f'norm{sim}' for sim in similarities] + \\\n","        [f'{sim}_clip-norm' if type(sim) is str else f'norm{sim}_clip-norm'\n","        for sim in similarities]\n","clip_norms = [False] * len(similarities) + [True] * len(similarities)\n","\n","# Double the length\n","similarities += similarities"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SL4o4doasH_k","executionInfo":{"status":"ok","timestamp":1702764936188,"user_tz":420,"elapsed":1885146,"user":{"displayName":"Nathan Schill","userId":"00230308692143417798"}},"outputId":"50e0d3af-b0fd-44b4-b8ba-ed2375f61e47"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------- dot --------------------------\n","  dog    :  dog Chapman straightened succumbed Women's as\n","  milk   :  milk flattered 1910 unstable sullen think\n","  run    :  run Lipton miserable managers ecstasy hazardous\n","  apple  :  apple to Friday recovery retired gown\n","  hurt   :  hurt 350 imagination machine welfare comparative\n","epoch: 1/5\n"," batch      0/4536 | loss 3.228\n"," batch   1500/4536 | loss 1.281\n"," batch   3000/4536 | loss 0.987\n"," batch   4500/4536 | loss 0.860\n","  dog    :  dog succumbed Without adolescents solidly Chapman\n","  milk   :  milk unstable flattered sullen Rice dozed\n","  run    :  run managers miserable grant Alabama ecstasy\n","  apple  :  apple recovery spacious Friday Interstate retired\n","  hurt   :  hurt welfare What's indicating indices torquer\n","epoch: 2/5\n"," batch      0/4536 | loss 0.718\n"," batch   1500/4536 | loss 0.685\n"," batch   3000/4536 | loss 0.659\n"," batch   4500/4536 | loss 0.625\n","  dog    :  dog Without solidly adolescents inspiring succumbed\n","  milk   :  milk dozed Rice flattered desegregated unstable\n","  run    :  managers grant recovery contraction styles miserable\n","  apple  :  apple recovery spacious utmost Interstate 56\n","  hurt   :  hurt indicating indices engulfed voluntarily What's\n","epoch: 3/5\n"," batch      0/4536 | loss 0.571\n"," batch   1500/4536 | loss 0.571\n"," batch   3000/4536 | loss 0.570\n"," batch   4500/4536 | loss 0.573\n","  dog    :  dog communist Without one-inch TSH fills\n","  milk   :  milk Rice dozed disposition flattered limbs\n","  run    :  battery grant contraction Doctrine cured wart\n","  apple  :  apple spacious utmost mounts recovery checkbook\n","  hurt   :  hurt indicating voluntarily scandals engulfed fever\n","epoch: 4/5\n"," batch      0/4536 | loss 0.543\n"," batch   1500/4536 | loss 0.533\n"," batch   3000/4536 | loss 0.536\n"," batch   4500/4536 | loss 0.538\n","  dog    :  dog communist solidly one-inch singer Without\n","  milk   :  milk dozed Rice limbs disposition signature\n","  run    :  battery arrears Doctrine vowed Slater wart\n","  apple  :  apple mounts assures glands spacious utmost\n","  hurt   :  hurt indicating sundown scandals fever punished\n","epoch: 5/5\n"," batch      0/4536 | loss 0.515\n"," batch   1500/4536 | loss 0.531\n"," batch   3000/4536 | loss 0.535\n"," batch   4500/4536 | loss 0.533\n","  dog    :  dog communist flags one-inch confronts wits\n","  milk   :  milk dozed signature disposition hollow 353\n","  run    :  battery ballplayer vowed arrears wart stretching\n","  apple  :  apple glands spacious mounts utmost deposited\n","  hurt   :  hurt scandals sundown indicating fever hips\n","\n","-------------------------- cos --------------------------\n","  dog    :  dog Chapman Women's yellow earnest doubtful\n","  milk   :  milk Szold flattered numbering sullen 1910\n","  run    :  run ecstasy wagon Very souls commissioner\n","  apple  :  apple How Friday spacious sugar recovery\n","  hurt   :  hurt conversation purified imagination 350 Frequently\n","epoch: 1/5\n"," batch      0/4536 | loss 0.693\n"," batch   1500/4536 | loss 0.668\n"," batch   3000/4536 | loss 0.634\n"," batch   4500/4536 | loss 0.617\n","  dog    :  dog American men off death English\n","  milk   :  milk job being money himself both\n","  run    :  run order or between other area\n","  apple  :  apple recovery spacious detected hunter water's\n","  hurt   :  hurt machine General beginning imagination conversation\n","epoch: 2/5\n"," batch      0/4536 | loss 0.609\n"," batch   1500/4536 | loss 0.600\n"," batch   3000/4536 | loss 0.597\n"," batch   4500/4536 | loss 0.587\n","  dog    :  dog death men work English people\n","  milk   :  milk control sound fiction cold English\n","  run    :  run work back set people came\n","  apple  :  apple recovery detected movable Nashville Quaker\n","  hurt   :  hurt machine answer approach dead right\n","epoch: 3/5\n"," batch      0/4536 | loss 0.593\n"," batch   1500/4536 | loss 0.592\n"," batch   3000/4536 | loss 0.586\n"," batch   4500/4536 | loss 0.592\n","  dog    :  dog return patient play hopes love\n","  milk   :  milk music old cold rock weather\n","  run    :  run work bed fight start return\n","  apple  :  apple recovery Security movable gown rivers\n","  hurt   :  hurt wait here kill mother returned\n","epoch: 4/5\n"," batch      0/4536 | loss 0.592\n"," batch   1500/4536 | loss 0.588\n"," batch   3000/4536 | loss 0.587\n"," batch   4500/4536 | loss 0.584\n","  dog    :  dog return feeling job claim patient\n","  milk   :  milk style music weather <UNK> yellow\n","  run    :  run watch drive start fight close\n","  apple  :  apple gown Security movable absorption representing\n","  hurt   :  hurt returned husband hurry here blame\n","epoch: 5/5\n"," batch      0/4536 | loss 0.588\n"," batch   1500/4536 | loss 0.593\n"," batch   3000/4536 | loss 0.590\n"," batch   4500/4536 | loss 0.587\n","  dog    :  dog way faith change sign return\n","  milk   :  milk cold wine yellow Red wood\n","  run    :  run set watch walk forward drive\n","  apple  :  apple gown absorption own Security movable\n","  hurt   :  hurt drink wait right husband returned\n","\n","------------------------- norm1 -------------------------\n","  dog    :  dog earnest Chapman doubtful yellow 18th\n","  milk   :  milk bronchioles 1/8'' retiring possibilities target\n","  run    :  run ecstasy wagon degrees Research Very\n","  apple  :  apple scored continuing How detected allowance\n","  hurt   :  hurt purified Bobbie foremost Death conversation\n","epoch: 1/5\n"," batch      0/4536 | loss 24.035\n"," batch   1500/4536 | loss 5.139\n"," batch   3000/4536 | loss 2.805\n"," batch   4500/4536 | loss 1.699\n","  dog    :  dog college caught From suddenly State\n","  milk   :  milk bring doing investigation Have man's\n","  run    :  run living least '' <UNK> and\n","  apple  :  apple Council pointed actual opening homes\n","  hurt   :  hurt moment except Mike couldn't especially\n","epoch: 2/5\n"," batch      0/4536 | loss 1.646\n"," batch   1500/4536 | loss 1.319\n"," batch   3000/4536 | loss 1.144\n"," batch   4500/4536 | loss 1.078\n","  dog    :  dog Yet in living the <UNK>\n","  milk   :  milk correct assistance whether distance well\n","  run    :  run felt here There had time\n","  apple  :  apple Republic adults votes consistent angle\n","  hurt   :  hurt openly Would Hans unlike regime\n","epoch: 3/5\n"," batch      0/4536 | loss 0.876\n"," batch   1500/4536 | loss 0.793\n"," batch   3000/4536 | loss 0.880\n"," batch   4500/4536 | loss 0.708\n","  dog    :  dog know take States its come\n","  milk   :  milk effort favorite from Therefore world\n","  run    :  run and an may had <UNK>\n","  apple  :  apple Ulyate generator shrugged endless 1949\n","  hurt   :  hurt I him when said that\n","epoch: 4/5\n"," batch      0/4536 | loss 0.710\n"," batch   1500/4536 | loss 0.681\n"," batch   3000/4536 | loss 0.672\n"," batch   4500/4536 | loss 0.664\n","  dog    :  dog any house general person <UNK>\n","  milk   :  milk material picture also all game\n","  run    :  run past which down for to\n","  apple  :  apple school flying gives from passed\n","  hurt   :  hurt wrote past own some as\n","epoch: 5/5\n"," batch      0/4536 | loss 0.660\n"," batch   1500/4536 | loss 0.682\n"," batch   3000/4536 | loss 0.666\n"," batch   4500/4536 | loss 0.648\n","  dog    :  dog with <UNK> a would has\n","  milk   :  milk certainly died place was visit\n","  run    :  run was to week , other\n","  apple  :  apple mankind requires probability fate David\n","  hurt   :  hurt mm. contest suitable century recordings\n","\n","------------------------- norm2 -------------------------\n","  dog    :  dog Chapman yellow dreary earnest doubtful\n","  milk   :  milk target Szold retiring intensity numbering\n","  run    :  run ecstasy wagon taxpayers Very lone\n","  apple  :  apple How water's scored spacious hunter\n","  hurt   :  hurt purified conversation Bobbie General pressing\n","epoch: 1/5\n"," batch      0/4536 | loss 3.756\n"," batch   1500/4536 | loss 1.176\n"," batch   3000/4536 | loss 0.887\n"," batch   4500/4536 | loss 0.751\n","  dog    :  dog college From caught current Saturday\n","  milk   :  milk bring machine Have investigation processes\n","  run    :  run least living effective needs matter\n","  apple  :  apple pointed sugar opening narrow integration\n","  hurt   :  hurt imagination Tom Hans Jr. machine\n","epoch: 2/5\n"," batch      0/4536 | loss 0.729\n"," batch   1500/4536 | loss 0.688\n"," batch   3000/4536 | loss 0.669\n"," batch   4500/4536 | loss 0.655\n","  dog    :  dog This possible as true no\n","  milk   :  milk correct bread shut nearby bride\n","  run    :  run walk attention standing left tomorrow\n","  apple  :  apple adults Ulyate emotion sugar votes\n","  hurt   :  hurt doctor's Hans openly Congressman unlike\n","epoch: 3/5\n"," batch      0/4536 | loss 0.629\n"," batch   1500/4536 | loss 0.612\n"," batch   3000/4536 | loss 0.622\n"," batch   4500/4536 | loss 0.598\n","  dog    :  dog seemed too find assumed now\n","  milk   :  milk name fashion passion road itself\n","  run    :  run day street mouth broke recognized\n","  apple  :  apple whip generator lectures fighters sadly\n","  hurt   :  hurt Hans Alex noticed dressed seen\n","epoch: 4/5\n"," batch      0/4536 | loss 0.607\n"," batch   1500/4536 | loss 0.591\n"," batch   3000/4536 | loss 0.594\n"," batch   4500/4536 | loss 0.593\n","  dog    :  dog others question bill transferred England\n","  milk   :  milk Yankees shoot coast degrees barn\n","  run    :  run solution promise another measurement sum\n","  apple  :  apple depending colleges leaves polished suits\n","  hurt   :  hurt son took keep tongue rose\n","epoch: 5/5\n"," batch      0/4536 | loss 0.588\n"," batch   1500/4536 | loss 0.604\n"," batch   3000/4536 | loss 0.597\n"," batch   4500/4536 | loss 0.590\n","  dog    :  dog alone money waiting included later\n","  milk   :  milk Above effects starting bottle host\n","  run    :  run warm down gift envelope beside\n","  apple  :  apple doors biwa inspection upward my\n","  hurt   :  hurt puts finally figured sought Wright\n","\n","------------------------- norm3 -------------------------\n","  dog    :  dog Chapman yellow apportionment complaint dreary\n","  milk   :  milk Szold target thunder theoretical sullen\n","  run    :  run ecstasy taxpayers wagon lone Very\n","  apple  :  apple How spacious hunter water's excitedly\n","  hurt   :  hurt purified conversation estates donor 1934\n","epoch: 1/5\n"," batch      0/4536 | loss 2.189\n"," batch   1500/4536 | loss 0.879\n"," batch   3000/4536 | loss 0.743\n"," batch   4500/4536 | loss 0.682\n","  dog    :  dog glance Saturday Jack study consider\n","  milk   :  milk drove bring theoretical Have U.\n","  run    :  run least effective times late question\n","  apple  :  apple narrow participation network actual sugar\n","  hurt   :  hurt Tom imagination Jr. Hans regime\n","epoch: 2/5\n"," batch      0/4536 | loss 0.663\n"," batch   1500/4536 | loss 0.641\n"," batch   3000/4536 | loss 0.634\n"," batch   4500/4536 | loss 0.623\n","  dog    :  dog best claim order carry figure\n","  milk   :  milk barn classical distribution swung sweet\n","  run    :  run fast long one Europe which\n","  apple  :  apple emotion foremost sugar Ulyate consciousness\n","  hurt   :  hurt Congressman Hans doctor's openly Would\n","epoch: 3/5\n"," batch      0/4536 | loss 0.610\n"," batch   1500/4536 | loss 0.599\n"," batch   3000/4536 | loss 0.601\n"," batch   4500/4536 | loss 0.593\n","  dog    :  dog turn sent attend recognized but\n","  milk   :  milk ground war society bottom black\n","  run    :  run nose counter seated mouth murder\n","  apple  :  apple marijuana obedience entertained allowances rigid\n","  hurt   :  hurt Alex Hans unlike Giffen noticed\n","epoch: 4/5\n"," batch      0/4536 | loss 0.598\n"," batch   1500/4536 | loss 0.587\n"," batch   3000/4536 | loss 0.589\n"," batch   4500/4536 | loss 0.587\n","  dog    :  dog headed others but historian fact\n","  milk   :  milk Spirit fish Oxford Yankees hotel\n","  run    :  run big Congo spirit promise moving\n","  apple  :  apple Instead decrease dots beam rigid\n","  hurt   :  hurt like here waiting angry Here\n","epoch: 5/5\n"," batch      0/4536 | loss 0.585\n"," batch   1500/4536 | loss 0.600\n"," batch   3000/4536 | loss 0.594\n"," batch   4500/4536 | loss 0.587\n","  dog    :  dog later country awareness changed matter\n","  milk   :  milk peasants compared center town blood\n","  run    :  run rolled sea heels pipe graduation\n","  apple  :  apple cracking rough into reflecting flowing\n","  hurt   :  hurt spoke then gave father too\n","\n","--------------------- dot_clip-norm ---------------------\n","  dog    :  dog okay eligible motel fibrosis harness\n","  milk   :  milk disinterested reading users Set buffer\n","  run    :  run fortunate gain trembling submit transaction\n","  apple  :  apple accessories witty expedition starting Askington\n","  hurt   :  hurt concept rookie staffs 1908 reportedly\n","epoch: 1/5\n"," batch      0/4536 | loss 3.190\n"," batch   1500/4536 | loss 1.237\n"," batch   3000/4536 | loss 0.915\n"," batch   4500/4536 | loss 0.768\n","  dog    :  dog okay eligible adverse Suppose fibrosis\n","  milk   :  milk disinterested typing explicitly scandal Set\n","  run    :  run fortunate afterward trembling muttered Orchestra\n","  apple  :  apple expedition Askington witty accessories retention\n","  hurt   :  hurt rookie staffs Karns 1908 Multnomah\n","epoch: 2/5\n"," batch      0/4536 | loss 0.753\n"," batch   1500/4536 | loss 0.677\n"," batch   3000/4536 | loss 0.641\n"," batch   4500/4536 | loss 0.628\n","  dog    :  dog okay telling guys Shelley width\n","  milk   :  milk disinterested Drexel explicitly typing prostitution\n","  run    :  afterward Orchestra Chapel Draft Sun DeKalb\n","  apple  :  apple expedition Askington prisoners Trevelyan's retention\n","  hurt   :  hurt rookie staffs Multnomah Colmer 1908\n","epoch: 3/5\n"," batch      0/4536 | loss 0.556\n"," batch   1500/4536 | loss 0.574\n"," batch   3000/4536 | loss 0.567\n"," batch   4500/4536 | loss 0.563\n","  dog    :  dog guys okay telling Bern Pathet\n","  milk   :  milk Drexel prostitution disinterested typing Ceylon\n","  run    :  afterward drill Orchestra Chapel diminished Sun\n","  apple  :  apple prisoners expedition Trevelyan's whereupon kidding\n","  hurt   :  hurt rookie canceled staffs Multnomah Colmer\n","epoch: 4/5\n"," batch      0/4536 | loss 0.534\n"," batch   1500/4536 | loss 0.524\n"," batch   3000/4536 | loss 0.530\n"," batch   4500/4536 | loss 0.532\n","  dog    :  dog Bern guys terrified Switzerland subsection\n","  milk   :  milk Drexel prostitution descending typing explicitly\n","  run    :  Lauderdale ignorance drill coconut afterward Max\n","  apple  :  apple prisoners Trevelyan's expedition kidding whereupon\n","  hurt   :  hurt rookie canceled staffs Multnomah bride\n","epoch: 5/5\n"," batch      0/4536 | loss 0.512\n"," batch   1500/4536 | loss 0.502\n"," batch   3000/4536 | loss 0.535\n"," batch   4500/4536 | loss 0.521\n","  dog    :  dog Switzerland guys Bern Korean eastern\n","  milk   :  milk Drexel prostitution descending typing explicitly\n","  run    :  drill Lauderdale upside ignorance coconut refusal\n","  apple  :  apple kidding skeptical Trevelyan's prisoners psychiatric\n","  hurt   :  hurt canceled rookie palette Oso Needham\n","\n","--------------------- cos_clip-norm ---------------------\n","  dog    :  dog eligible okay motel change ever\n","  milk   :  milk nail disinterested Set Da continuity\n","  run    :  run sunlight When 1922 transaction steep\n","  apple  :  apple accessories ambitions witty Askington evasive\n","  hurt   :  hurt concept rookie guidance staffs ragged\n","epoch: 1/5\n"," batch      0/4536 | loss 0.695\n"," batch   1500/4536 | loss 0.670\n"," batch   3000/4536 | loss 0.638\n"," batch   4500/4536 | loss 0.618\n","  dog    :  dog change ever hope day about\n","  milk   :  milk disinterested event nail artery helped\n","  run    :  run way fact world part both\n","  apple  :  apple accessories Askington ambitions witty expedition\n","  hurt   :  hurt decisions comfortable able screaming rookie\n","epoch: 2/5\n"," batch      0/4536 | loss 0.615\n"," batch   1500/4536 | loss 0.604\n"," batch   3000/4536 | loss 0.595\n"," batch   4500/4536 | loss 0.594\n","  dog    :  dog change way hope word return\n","  milk   :  milk gas peace River domestic music\n","  run    :  run set work back out way\n","  apple  :  apple ambitions Askington sandy expedition accessories\n","  hurt   :  hurt decisions hit placed watched here\n","epoch: 3/5\n"," batch      0/4536 | loss 0.587\n"," batch   1500/4536 | loss 0.588\n"," batch   3000/4536 | loss 0.585\n"," batch   4500/4536 | loss 0.586\n","  dog    :  dog claim return time job matter\n","  milk   :  milk <UNK> gas intellectual First music\n","  run    :  run set walk hold moved turn\n","  apple  :  apple fires ambitions retention expedition crack\n","  hurt   :  hurt hit raised built brought left\n","epoch: 4/5\n"," batch      0/4536 | loss 0.585\n"," batch   1500/4536 | loss 0.582\n"," batch   3000/4536 | loss 0.584\n"," batch   4500/4536 | loss 0.582\n","  dog    :  dog note return job feeling way\n","  milk   :  milk wine Italian former <UNK> music\n","  run    :  run watch fight walk play left\n","  apple  :  apple fires ambitions retention expedition Fulton\n","  hurt   :  hurt raised hit right returned here\n","epoch: 5/5\n"," batch      0/4536 | loss 0.586\n"," batch   1500/4536 | loss 0.582\n"," batch   3000/4536 | loss 0.590\n"," batch   4500/4536 | loss 0.586\n","  dog    :  dog job return change dream faith\n","  milk   :  milk wine style station comedy weather\n","  run    :  run walk dropped start set break\n","  apple  :  apple fires socialist retention closet ambitions\n","  hurt   :  hurt caught husband returned stop sleep\n","\n","-------------------- norm1_clip-norm --------------------\n","  dog    :  dog eligible plants Christian officials change\n","  milk   :  milk nail continuity histories thumping gratitude\n","  run    :  run sunlight location necessary strings cream\n","  apple  :  apple cathode ambitions examiner slower Gross\n","  hurt   :  hurt Willis guidance Music Commonwealth magnetic\n","epoch: 1/5\n"," batch      0/4536 | loss 24.271\n"," batch   1500/4536 | loss 5.491\n"," batch   3000/4536 | loss 2.324\n"," batch   4500/4536 | loss 1.530\n","  dog    :  dog ran clear countries as us\n","  milk   :  milk straight cannot Kennedy company ran\n","  run    :  run nature him through the <UNK>\n","  apple  :  apple torn decide security profession cathode\n","  hurt   :  hurt Well charge cent father shelter\n","epoch: 2/5\n"," batch      0/4536 | loss 1.653\n"," batch   1500/4536 | loss 1.316\n"," batch   3000/4536 | loss 1.003\n"," batch   4500/4536 | loss 0.962\n","  dog    :  dog the all <UNK> this woman\n","  milk   :  milk normal account The great at\n","  run    :  run most <UNK> development not a\n","  apple  :  apple optimistic blond limitations diplomatic Morse\n","  hurt   :  hurt 13 newspapers families assured guidance\n","epoch: 3/5\n"," batch      0/4536 | loss 0.820\n"," batch   1500/4536 | loss 0.901\n"," batch   3000/4536 | loss 0.757\n"," batch   4500/4536 | loss 0.716\n","  dog    :  dog Be reported come arms then\n","  milk   :  milk away appear All last themselves\n","  run    :  run face we something as now\n","  apple  :  apple deliberations Oklahoma commerce hospitals ties\n","  hurt   :  hurt built capital cities crossed hay\n","epoch: 4/5\n"," batch      0/4536 | loss 0.682\n"," batch   1500/4536 | loss 0.703\n"," batch   3000/4536 | loss 0.692\n"," batch   4500/4536 | loss 0.659\n","  dog    :  dog right its course men a\n","  milk   :  milk behind . North down on\n","  run    :  run of world <UNK> to is\n","  apple  :  apple deliberations Oklahoma snarled commerce blond\n","  hurt   :  hurt weapons place looking press river\n","epoch: 5/5\n"," batch      0/4536 | loss 0.681\n"," batch   1500/4536 | loss 0.670\n"," batch   3000/4536 | loss 0.648\n"," batch   4500/4536 | loss 0.651\n","  dog    :  dog he the a <UNK> him\n","  milk   :  milk still This little day didn't\n","  run    :  run he which <UNK> the that\n","  apple  :  apple cooler admitted important with he\n","  hurt   :  hurt statement could women something must\n","\n","-------------------- norm2_clip-norm --------------------\n","  dog    :  dog eligible change officials greeted Andrew\n","  milk   :  milk nail continuity Set thumping jig\n","  run    :  run sunlight location strings rattlesnakes When\n","  apple  :  apple cathode ambitions 500 examiner accessories\n","  hurt   :  hurt guidance Music Willis planet dressing\n","epoch: 1/5\n"," batch      0/4536 | loss 3.779\n"," batch   1500/4536 | loss 1.230\n"," batch   3000/4536 | loss 0.816\n"," batch   4500/4536 | loss 0.719\n","  dog    :  dog ran countries clear required All\n","  milk   :  milk standard peace helped cast board\n","  run    :  run this next were now him\n","  apple  :  apple P theater cathode troubled Pamela\n","  hurt   :  hurt concept Well comfortable despite Such\n","epoch: 2/5\n"," batch      0/4536 | loss 0.733\n"," batch   1500/4536 | loss 0.690\n"," batch   3000/4536 | loss 0.639\n"," batch   4500/4536 | loss 0.639\n","  dog    :  dog firm son being war facts\n","  milk   :  milk wood all thick radio quick\n","  run    :  run cattle time -- thing ,\n","  apple  :  apple blond deliberations Oklahoma ties Fulton\n","  hurt   :  hurt decide we'll mentioned remembered we\n","epoch: 3/5\n"," batch      0/4536 | loss 0.615\n"," batch   1500/4536 | loss 0.625\n"," batch   3000/4536 | loss 0.608\n"," batch   4500/4536 | loss 0.595\n","  dog    :  dog narrow putting running his gets\n","  milk   :  milk escape tells long Indeed all\n","  run    :  run man thinking long one teeth\n","  apple  :  apple deliberations Oklahoma regretted therein Papa-san\n","  hurt   :  hurt velocity would deny if matter\n","epoch: 4/5\n"," batch      0/4536 | loss 0.590\n"," batch   1500/4536 | loss 0.593\n"," batch   3000/4536 | loss 0.598\n"," batch   4500/4536 | loss 0.585\n","  dog    :  dog course chair town master especially\n","  milk   :  milk needed given known others direction\n","  run    :  run laid barn dogs look boy\n","  apple  :  apple snarled deliberations Oklahoma Papa-san therein\n","  hurt   :  hurt sir Wait Then surprised fallen\n","epoch: 5/5\n"," batch      0/4536 | loss 0.591\n"," batch   1500/4536 | loss 0.589\n"," batch   3000/4536 | loss 0.594\n"," batch   4500/4536 | loss 0.594\n","  dog    :  dog appear possible saved that reason\n","  milk   :  milk number account characters members considerable\n","  run    :  run keep thick After neck cut\n","  apple  :  apple deliberations snarled Papa-san limb cooler\n","  hurt   :  hurt Curt down bent waved Go\n","\n","-------------------- norm3_clip-norm --------------------\n","  dog    :  dog eligible change officials agreements greeted\n","  milk   :  milk nail Set continuity jig smoothness\n","  run    :  run sunlight rattlesnakes dripped When 1922\n","  apple  :  apple cathode ambitions Shu accessories attendants\n","  hurt   :  hurt proving protests guidance outline taxpayers\n","epoch: 1/5\n"," batch      0/4536 | loss 2.197\n"," batch   1500/4536 | loss 0.909\n"," batch   3000/4536 | loss 0.706\n"," batch   4500/4536 | loss 0.663\n","  dog    :  dog material economic married alone future\n","  milk   :  milk peace decided helped board volume\n","  run    :  run now face man is possible\n","  apple  :  apple theater doorway Fulton P troubled\n","  hurt   :  hurt concept comfortable Such Well continue\n","epoch: 2/5\n"," batch      0/4536 | loss 0.667\n"," batch   1500/4536 | loss 0.645\n"," batch   3000/4536 | loss 0.612\n"," batch   4500/4536 | loss 0.616\n","  dog    :  dog food son meeting war point\n","  milk   :  milk contained credit power experience opinion\n","  run    :  run fight cause job her then\n","  apple  :  apple deliberations blond regretted rides Oklahoma\n","  hurt   :  hurt conclusions comfortable Before mentioned superior\n","epoch: 3/5\n"," batch      0/4536 | loss 0.601\n"," batch   1500/4536 | loss 0.606\n"," batch   3000/4536 | loss 0.596\n"," batch   4500/4536 | loss 0.586\n","  dog    :  dog putting congregation money play fight\n","  milk   :  milk dress however camera career strike\n","  run    :  run long car teeth brought hold\n","  apple  :  apple deliberations Oklahoma snarled regretted therein\n","  hurt   :  hurt left still taken There spend\n","epoch: 4/5\n"," batch      0/4536 | loss 0.584\n"," batch   1500/4536 | loss 0.586\n"," batch   3000/4536 | loss 0.592\n"," batch   4500/4536 | loss 0.581\n","  dog    :  dog play hands jury entered settlement\n","  milk   :  milk party youth time fine however\n","  run    :  run around voice back hair dogs\n","  apple  :  apple deliberations snarled Oklahoma Papa-san subspace\n","  hurt   :  hurt ride stay right go Take\n","epoch: 5/5\n"," batch      0/4536 | loss 0.586\n"," batch   1500/4536 | loss 0.583\n"," batch   3000/4536 | loss 0.590\n"," batch   4500/4536 | loss 0.590\n","  dog    :  dog matter those act doctor job\n","  milk   :  milk range life public plane editor\n","  run    :  run pay left set ahead lot\n","  apple  :  apple deliberations snarled limb cooler Papa-san\n","  hurt   :  hurt stood up away Henrietta off\n","\n"]}],"source":["dataset = Samples(contexts, negative_samples)\n","\n","# For each model type, list of average batch loss for each epoch\n","avg_epoch_losses = dict()\n","\n","for name, similarity, clip_norm in zip(names, similarities, clip_norms):\n","  print(f\"{' ' + name + ' ':-^57}\")\n","\n","  torch.manual_seed(0)\n","  model = CustomEmbedding(vocab, similarity=similarity,\n","                          embedding_dim=embedding_dim, clip_norm=clip_norm) \\\n","                          .to(device)\n","\n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  dataloader = DataLoader(\n","      dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch,\n","  )\n","\n","  losses = train_skipgram(model,\n","                          criterion,\n","                          optimizer,\n","                          dataloader,\n","                          num_epochs = num_epochs,\n","                          topk=topk,)\n","\n","  # abs_model_path = os.path.join(DRIVE_PATH, embedding_folder_path, name + '.pt')\n","  # torch.save(model.state_dict(), abs_model_path)\n","\n","  avg_epoch_losses[name] = losses\n","\n","  print()\n","\n","with open(os.path.join(DRIVE_PATH, epoch_losses_path + '.json'), 'w') as file:\n","  json.dump(avg_epoch_losses, file)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPfTWNFVfhT/b5G/IfmTHy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}